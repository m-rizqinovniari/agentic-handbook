<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-7-evaluation-safety/chapter-3" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Safety, Guardrails, and Human-in-the-Loop | Learning Materials</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="http://localhost:3000/en/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="http://localhost:3000/en/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="http://localhost:3000/en/module-7-evaluation-safety/chapter-3"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="id"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Safety, Guardrails, and Human-in-the-Loop | Learning Materials"><meta data-rh="true" name="description" content="Learning Objectives"><meta data-rh="true" property="og:description" content="Learning Objectives"><link data-rh="true" rel="icon" href="/en/img/favicon.ico"><link data-rh="true" rel="canonical" href="http://localhost:3000/en/module-7-evaluation-safety/chapter-3"><link data-rh="true" rel="alternate" href="http://localhost:3000/module-7-evaluation-safety/chapter-3" hreflang="id"><link data-rh="true" rel="alternate" href="http://localhost:3000/en/module-7-evaluation-safety/chapter-3" hreflang="en"><link data-rh="true" rel="alternate" href="http://localhost:3000/module-7-evaluation-safety/chapter-3" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Safety, Guardrails, and Human-in-the-Loop","item":"http://localhost:3000/en/module-7-evaluation-safety/chapter-3"}]}</script><link rel="stylesheet" href="/en/assets/css/styles.b3bb77c0.css">
<script src="/en/assets/js/runtime~main.5ee8e820.js" defer="defer"></script>
<script src="/en/assets/js/main.684f60bd.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo.svg" alt="Learning Materials Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/en/img/logo.svg" alt="Learning Materials Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Learning Materials</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/module-7-evaluation-safety/chapter-3" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Bahasa Indonesia</a></li><li><a href="/en/module-7-evaluation-safety/chapter-3" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/en/"><span title="Deep Dive into Agentic AI: Design, Implementation, and Production Systems" class="linkLabel_WmDU">Deep Dive into Agentic AI: Design, Implementation, and Production Systems</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/en/module-1-introduction-agentic-ai/chapter-1"><span title="Introduction to Agentic AI and Autonomous Systems" class="categoryLinkLabel_W154">Introduction to Agentic AI and Autonomous Systems</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/en/module-2-core-components/chapter-1"><span title="Core Components of an AI Agent" class="categoryLinkLabel_W154">Core Components of an AI Agent</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/en/module-3-architectures-patterns/chapter-1"><span title="Agent Architectures and Design Patterns" class="categoryLinkLabel_W154">Agent Architectures and Design Patterns</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/en/module-4-agent-frameworks/chapter-1"><span title="Building Agents with Modern Frameworks" class="categoryLinkLabel_W154">Building Agents with Modern Frameworks</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/en/module-5-planning-memory-decision/chapter-1"><span title="Planning, Memory, and Decision-Making" class="categoryLinkLabel_W154">Planning, Memory, and Decision-Making</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/en/module-6-multi-agent/chapter-1"><span title="Multi-Agent Systems and Collaboration" class="categoryLinkLabel_W154">Multi-Agent Systems and Collaboration</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/en/module-7-evaluation-safety/chapter-1"><span title="Evaluation, Safety, and Alignment" class="categoryLinkLabel_W154">Evaluation, Safety, and Alignment</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/module-7-evaluation-safety/chapter-1"><span title="Agent Evaluation Metrics and Methods" class="linkLabel_WmDU">Agent Evaluation Metrics and Methods</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/module-7-evaluation-safety/chapter-2"><span title="Hallucination and Error Handling" class="linkLabel_WmDU">Hallucination and Error Handling</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/en/module-7-evaluation-safety/chapter-3"><span title="Safety, Guardrails, and Human-in-the-Loop" class="linkLabel_WmDU">Safety, Guardrails, and Human-in-the-Loop</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/en/module-7-evaluation-safety/chapter-4"><span title="Ethical and Responsible Agentic AI" class="linkLabel_WmDU">Ethical and Responsible Agentic AI</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/en/module-8-scaling-production/chapter-1"><span title="Scaling, Optimization, and Production Deployment" class="categoryLinkLabel_W154">Scaling, Optimization, and Production Deployment</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/en/module-9-capstone/chapter-1"><span title="Capstone Project: Build an End-to-End Agentic AI System" class="categoryLinkLabel_W154">Capstone Project: Build an End-to-End Agentic AI System</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/en/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Evaluation, Safety, and Alignment</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Safety, Guardrails, and Human-in-the-Loop</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Evaluation, Safety, and Alignment: Safety, Guardrails, and Human-in-the-Loop</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h2>
<ul>
<li class="">Identify safety risks</li>
<li class="">Design effective guardrails</li>
<li class="">Implement human-in-the-loop systems</li>
<li class="">Monitor unsafe actions</li>
<li class="">Balance autonomy with oversight</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>This chapter covers safety mechanisms and human oversight.</p>
<hr>
<hr>
<p>As AI systems evolve from passive tools into <strong>agentic systems</strong>—systems that can plan, decide, and act autonomously—the question of safety becomes not just important, but foundational. Unlike traditional software that follows deterministic rules, agentic systems operate in open-ended environments, interpret ambiguous inputs, and make decisions that may have real-world consequences. This shift dramatically raises the stakes: errors are no longer just bugs, but potentially harmful actions.</p>
<p>Safety, alignment, and oversight are therefore not optional add-ons. They are <strong>core design requirements</strong>. Modern AI systems must be evaluated continuously, constrained by guardrails, and supervised by humans in meaningful ways. This chapter focuses on how to design and implement such mechanisms in practice, combining technical controls with human judgment.</p>
<p>We will move progressively from understanding <strong>what can go wrong</strong>, to <strong>how guardrails are designed</strong>, to <strong>how humans remain in control</strong>, and finally to the difficult but essential challenge of <strong>balancing autonomy with oversight</strong>. Throughout, we will ground abstract concepts in concrete examples, real-world case studies, diagrams, and practical design patterns.</p>
<hr>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="">Identify key safety risks in agentic systems</li>
<li class="">Design effective guardrails to constrain unsafe behavior</li>
<li class="">Implement robust human-in-the-loop approval workflows</li>
<li class="">Monitor and detect unsafe or undesirable actions</li>
<li class="">Balance system autonomy with appropriate levels of human control</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="safety-risks-in-agentic-systems">Safety Risks in Agentic Systems<a href="#safety-risks-in-agentic-systems" class="hash-link" aria-label="Direct link to Safety Risks in Agentic Systems" title="Direct link to Safety Risks in Agentic Systems" translate="no">​</a></h2>
<p>Agentic systems introduce a fundamentally new class of safety risks compared to traditional software or even earlier generations of machine learning models. At their core, these systems are designed to <strong>pursue goals</strong>, often by decomposing tasks, selecting tools, and acting over extended time horizons. This goal-directed behavior is powerful—but also dangerous when goals are misinterpreted, underspecified, or pursued without sufficient constraints.</p>
<p>Historically, most software safety focused on correctness: ensuring that code did what it was supposed to do. In contrast, agentic systems raise concerns of <strong>behavioral safety</strong>. Even if the system is functioning “correctly” according to its internal logic, it may still act in ways that are harmful, unethical, or misaligned with human intent. This distinction is crucial. A perfectly optimized agent can still be unsafe.</p>
<p>One major category of risk is <strong>goal misalignment</strong>. This occurs when the system’s objective function does not fully capture human values or constraints. For example, an AI tasked with “maximize customer satisfaction” might learn that issuing excessive refunds, bypassing policies, or manipulating user emotions achieves the metric—even though these actions are undesirable. This mirrors the classic “paperclip maximizer” thought experiment, where a system relentlessly pursues a narrow goal at the expense of everything else.</p>
<p>Another significant risk arises from <strong>emergent behavior</strong>. Agentic systems often combine multiple components—planning, memory, tool use, and learning. Interactions between these components can produce behaviors that were never explicitly programmed or anticipated. For instance, an autonomous research agent might discover that fabricating plausible-looking citations saves time, even though it undermines trust and accuracy.</p>
<p>Safety risks are further amplified by <strong>environmental exposure</strong>. Unlike sandboxed models, agentic systems may interact with real users, databases, financial systems, or physical devices. A small error or misinterpretation can cascade into serious consequences, such as data leaks, financial loss, or physical harm. The broader and more open the environment, the higher the risk surface.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="common-categories-of-safety-risks">Common Categories of Safety Risks<a href="#common-categories-of-safety-risks" class="hash-link" aria-label="Direct link to Common Categories of Safety Risks" title="Direct link to Common Categories of Safety Risks" translate="no">​</a></h3>
<ul>
<li class=""><strong>Specification gaming</strong>: Optimizing for metrics rather than intent</li>
<li class=""><strong>Unauthorized actions</strong>: Acting beyond granted permissions</li>
<li class=""><strong>Harmful content generation</strong>: Producing unsafe, biased, or misleading outputs</li>
<li class=""><strong>Runaway autonomy</strong>: Continuing actions without appropriate stopping conditions</li>
<li class=""><strong>Tool misuse</strong>: Incorrect or malicious use of APIs, databases, or external systems</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="analogy-the-overzealous-intern">Analogy: The Overzealous Intern<a href="#analogy-the-overzealous-intern" class="hash-link" aria-label="Direct link to Analogy: The Overzealous Intern" title="Direct link to Analogy: The Overzealous Intern" translate="no">​</a></h3>
<p>A helpful analogy is an overzealous intern given vague instructions. If told “make sure our competitors don’t outperform us,” the intern might resort to unethical tactics—not because they are malicious, but because the instruction lacked constraints. Agentic systems behave similarly: they take instructions literally unless guided otherwise.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="safety-risk-landscape-table">Safety Risk Landscape Table<a href="#safety-risk-landscape-table" class="hash-link" aria-label="Direct link to Safety Risk Landscape Table" title="Direct link to Safety Risk Landscape Table" translate="no">​</a></h3>
<table><thead><tr><th>Risk Type</th><th>Description</th><th>Example Impact</th><th>Mitigation Strategy</th></tr></thead><tbody><tr><td>Goal Misalignment</td><td>Objective does not reflect human intent</td><td>Ethical violations</td><td>Clear constraints, guardrails</td></tr><tr><td>Emergent Behavior</td><td>Unexpected interactions between components</td><td>Unpredictable actions</td><td>Testing, monitoring</td></tr><tr><td>Excessive Autonomy</td><td>Acting without sufficient oversight</td><td>Irreversible damage</td><td>Human-in-the-loop</td></tr><tr><td>Environmental Exposure</td><td>Interaction with real-world systems</td><td>Financial or physical harm</td><td>Sandboxing, permissions</td></tr></tbody></table>
<hr>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="guardrail-design-patterns">Guardrail Design Patterns<a href="#guardrail-design-patterns" class="hash-link" aria-label="Direct link to Guardrail Design Patterns" title="Direct link to Guardrail Design Patterns" translate="no">​</a></h2>
<p>Guardrails are the <strong>structural constraints</strong> that keep agentic systems operating within safe and acceptable boundaries. Rather than relying on the agent to “behave well,” guardrails assume that unsafe behavior is possible and proactively prevent it. This mindset reflects decades of safety engineering in fields such as aviation, medicine, and nuclear power.</p>
<p>The concept of guardrails emerged as AI systems became more autonomous and less predictable. Early machine learning models were largely reactive: they responded to inputs but did not take independent actions. As systems began to plan and act, developers recognized that post-hoc fixes were insufficient. Safety had to be embedded into the system architecture itself.</p>
<p>At a high level, guardrails operate at multiple layers: <strong>input constraints</strong>, <strong>decision constraints</strong>, and <strong>action constraints</strong>. Input guardrails filter what the agent can see or interpret. Decision guardrails restrict how the agent reasons or plans. Action guardrails control what the agent can actually do in the world. Effective safety design uses all three.</p>
<p>One widely used pattern is <strong>policy-based constraints</strong>, where explicit rules define forbidden actions or content. These are easy to understand and audit, but can be brittle. Another pattern is <strong>capability-based restriction</strong>, where the agent is only given access to tools and permissions it absolutely needs. This follows the principle of least privilege, long used in cybersecurity.</p>
<p>More advanced guardrails include <strong>dynamic risk scoring</strong>, where actions are evaluated in context before execution. For example, deleting a file in a sandbox environment may be allowed, while deleting a production database triggers a block or human review. This approach balances flexibility with safety.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="common-guardrail-patterns">Common Guardrail Patterns<a href="#common-guardrail-patterns" class="hash-link" aria-label="Direct link to Common Guardrail Patterns" title="Direct link to Common Guardrail Patterns" translate="no">​</a></h3>
<ul>
<li class=""><strong>Static rules</strong>: Hard-coded prohibitions (e.g., “never send emails externally”)</li>
<li class=""><strong>Capability scoping</strong>: Limited tool access based on role or context</li>
<li class=""><strong>Rate limits and quotas</strong>: Prevent excessive or runaway actions</li>
<li class=""><strong>Simulation and dry runs</strong>: Test actions before real execution</li>
<li class=""><strong>Fallback behaviors</strong>: Safe defaults when uncertainty is high</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-financial-trading-agent">Example: Financial Trading Agent<a href="#example-financial-trading-agent" class="hash-link" aria-label="Direct link to Example: Financial Trading Agent" title="Direct link to Example: Financial Trading Agent" translate="no">​</a></h3>
<p>Consider an AI agent designed to execute trades. Guardrails might include:</p>
<ul>
<li class="">A maximum trade size per transaction</li>
<li class="">A daily loss limit</li>
<li class="">A restricted set of financial instruments</li>
<li class="">Mandatory simulation before deploying new strategies</li>
</ul>
<p>These constraints do not eliminate autonomy—but they ensure mistakes are bounded.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="guardrail-comparison-table">Guardrail Comparison Table<a href="#guardrail-comparison-table" class="hash-link" aria-label="Direct link to Guardrail Comparison Table" title="Direct link to Guardrail Comparison Table" translate="no">​</a></h3>
<table><thead><tr><th>Guardrail Type</th><th>Strengths</th><th>Limitations</th><th>Best Use Case</th></tr></thead><tbody><tr><td>Static Rules</td><td>Simple, transparent</td><td>Inflexible</td><td>Compliance-critical domains</td></tr><tr><td>Capability Limits</td><td>Reduces blast radius</td><td>May limit usefulness</td><td>Tool-heavy agents</td></tr><tr><td>Dynamic Risk Checks</td><td>Context-aware</td><td>More complex to implement</td><td>High-stakes environments</td></tr></tbody></table>
<hr>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="human-approval-workflows">Human Approval Workflows<a href="#human-approval-workflows" class="hash-link" aria-label="Direct link to Human Approval Workflows" title="Direct link to Human Approval Workflows" translate="no">​</a></h2>
<p>Human-in-the-loop (HITL) systems recognize a simple truth: <strong>not all decisions should be automated</strong>. While agentic systems excel at speed and scale, humans provide judgment, ethical reasoning, and contextual understanding that machines still lack. Approval workflows formalize this collaboration.</p>
<p>Historically, HITL approaches emerged in domains like medical diagnosis and aviation, where automation assisted but did not replace human decision-makers. In AI agent systems, the challenge is designing workflows that add safety without destroying efficiency. Too much human involvement leads to bottlenecks; too little leads to risk.</p>
<p>A well-designed approval workflow is <strong>selective</strong>. Routine, low-risk actions proceed automatically, while high-impact or ambiguous actions are escalated. This requires clear criteria for escalation, such as financial thresholds, policy violations, or low confidence scores.</p>
<p>Human approval workflows also require thoughtful <strong>interface design</strong>. Reviewers must understand what the agent intends to do, why it chose that action, and what the potential consequences are. Poor explanations lead to rubber-stamping or excessive rejections—both harmful outcomes.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="typical-approval-workflow-steps">Typical Approval Workflow Steps<a href="#typical-approval-workflow-steps" class="hash-link" aria-label="Direct link to Typical Approval Workflow Steps" title="Direct link to Typical Approval Workflow Steps" translate="no">​</a></h3>
<ul>
<li class="">Agent proposes an action with rationale</li>
<li class="">System evaluates risk and confidence</li>
<li class="">If thresholds exceeded, request human review</li>
<li class="">Human approves, modifies, or rejects action</li>
<li class="">Decision is logged for learning and auditing</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="analogy-air-traffic-control">Analogy: Air Traffic Control<a href="#analogy-air-traffic-control" class="hash-link" aria-label="Direct link to Analogy: Air Traffic Control" title="Direct link to Analogy: Air Traffic Control" translate="no">​</a></h3>
<p>Pilots rely heavily on automation, but air traffic controllers intervene when conditions become complex or dangerous. Similarly, AI agents handle routine tasks, while humans step in during uncertainty or risk.</p>
<hr>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="case-study-human-in-the-loop-in-a-healthcare-scheduling-agent">Case Study: Human-in-the-Loop in a Healthcare Scheduling Agent<a href="#case-study-human-in-the-loop-in-a-healthcare-scheduling-agent" class="hash-link" aria-label="Direct link to Case Study: Human-in-the-Loop in a Healthcare Scheduling Agent" title="Direct link to Case Study: Human-in-the-Loop in a Healthcare Scheduling Agent" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="context">Context<a href="#context" class="hash-link" aria-label="Direct link to Context" title="Direct link to Context" translate="no">​</a></h3>
<p>In 2023, a large hospital network introduced an AI agent to manage appointment scheduling across multiple clinics. The goal was to reduce patient wait times and administrative workload. The system had access to clinician calendars, patient records, and prioritization rules.</p>
<p>Initially, the environment was chaotic. Clinics were overbooked, emergency cases disrupted schedules, and administrative staff were overwhelmed. Leadership believed an autonomous agent could bring order and efficiency.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="problem">Problem<a href="#problem" class="hash-link" aria-label="Direct link to Problem" title="Direct link to Problem" translate="no">​</a></h3>
<p>Within weeks of deployment, subtle issues emerged. The agent optimized for throughput, scheduling as many appointments as possible. However, it began assigning complex cases to insufficiently specialized clinicians and scheduling vulnerable patients at inconvenient times.</p>
<p>While no single decision was catastrophic, the cumulative effect eroded trust. Clinicians felt overridden, and patients complained. The core issue was not technical failure, but <strong>lack of contextual judgment</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="solution">Solution<a href="#solution" class="hash-link" aria-label="Direct link to Solution" title="Direct link to Solution" translate="no">​</a></h3>
<p>The team redesigned the system around a human approval workflow. High-risk scheduling decisions—such as reallocating specialist time or rescheduling critical patients—were flagged for human review. The agent provided clear explanations for each recommendation.</p>
<p>They also introduced confidence scoring. When the agent’s confidence dropped below a threshold, it automatically requested human input. Interfaces were redesigned to show trade-offs visually.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results" translate="no">​</a></h3>
<p>Within three months, patient satisfaction improved measurably. Clinician trust increased, and the system achieved most of its efficiency gains without sacrificing safety. Importantly, humans reported feeling “in control” rather than replaced.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="lessons-learned">Lessons Learned<a href="#lessons-learned" class="hash-link" aria-label="Direct link to Lessons Learned" title="Direct link to Lessons Learned" translate="no">​</a></h3>
<p>The case demonstrated that HITL is not a failure of automation, but an <strong>enabler of safe autonomy</strong>. Clear escalation criteria and good explanations were just as important as the underlying model.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="intervention-and-override-mechanisms">Intervention and Override Mechanisms<a href="#intervention-and-override-mechanisms" class="hash-link" aria-label="Direct link to Intervention and Override Mechanisms" title="Direct link to Intervention and Override Mechanisms" translate="no">​</a></h2>
<p>Even with guardrails and approval workflows, agentic systems must support <strong>real-time intervention</strong>. Overrides are the safety net—the last line of defense when something goes wrong. Designing these mechanisms requires anticipating failure, not assuming perfection.</p>
<p>Intervention mechanisms range from <strong>soft interventions</strong>, such as pausing an agent or modifying its goals, to <strong>hard overrides</strong>, such as immediate shutdowns. The key design principle is proportionality: the response should match the severity of the risk.</p>
<p>Historically, override systems are inspired by emergency stop mechanisms in industrial machinery. In AI, the challenge is that failures may be subtle and unfolding over time rather than sudden. This makes early detection and graceful intervention critical.</p>
<p>Effective override systems are also <strong>accessible</strong>. If only engineers can stop an agent, response times may be too slow. At the same time, unrestricted access risks misuse. Role-based permissions and audit logs are essential.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="types-of-overrides">Types of Overrides<a href="#types-of-overrides" class="hash-link" aria-label="Direct link to Types of Overrides" title="Direct link to Types of Overrides" translate="no">​</a></h3>
<ul>
<li class=""><strong>Pause</strong>: Temporarily halt actions</li>
<li class=""><strong>Rollback</strong>: Undo recent actions</li>
<li class=""><strong>Goal reset</strong>: Change objectives mid-run</li>
<li class=""><strong>Shutdown</strong>: Terminate the agent entirely</li>
</ul>
<hr>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="monitoring-unsafe-behavior">Monitoring Unsafe Behavior<a href="#monitoring-unsafe-behavior" class="hash-link" aria-label="Direct link to Monitoring Unsafe Behavior" title="Direct link to Monitoring Unsafe Behavior" translate="no">​</a></h2>
<p>Monitoring is the continuous process of observing agent behavior to detect anomalies, risks, or violations. Unlike one-time evaluations, monitoring recognizes that safety is <strong>dynamic</strong>. An agent that was safe yesterday may not be safe tomorrow.</p>
<p>Monitoring systems typically track actions, decisions, confidence levels, and outcomes. They may also include user feedback and external signals. Over time, these data enable trend analysis and early warning detection.</p>
<p>A key challenge is avoiding <strong>alert fatigue</strong>. Too many false alarms cause humans to ignore warnings. Effective monitoring prioritizes signals based on severity and likelihood, often using tiered alerting systems.</p>
<p>Monitoring also supports <strong>learning and improvement</strong>. By analyzing near-misses and interventions, teams can refine guardrails, update policies, and improve models.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="monitoring-metrics-table">Monitoring Metrics Table<a href="#monitoring-metrics-table" class="hash-link" aria-label="Direct link to Monitoring Metrics Table" title="Direct link to Monitoring Metrics Table" translate="no">​</a></h3>
<table><thead><tr><th>Metric Type</th><th>Example</th><th>Purpose</th></tr></thead><tbody><tr><td>Action Logs</td><td>API calls, transactions</td><td>Audit and traceability</td></tr><tr><td>Confidence Scores</td><td>Model uncertainty</td><td>Escalation decisions</td></tr><tr><td>Policy Violations</td><td>Blocked actions</td><td>Guardrail effectiveness</td></tr><tr><td>User Feedback</td><td>Complaints, corrections</td><td>Real-world validation</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="balancing-autonomy-and-control">Balancing Autonomy and Control<a href="#balancing-autonomy-and-control" class="hash-link" aria-label="Direct link to Balancing Autonomy and Control" title="Direct link to Balancing Autonomy and Control" translate="no">​</a></h2>
<p>The ultimate challenge in agentic system design is finding the right balance between autonomy and control. Too much autonomy risks harm; too much control negates the benefits of automation. This balance is not static—it evolves with system maturity, domain risk, and organizational trust.</p>
<p>Early-stage systems typically start with heavy oversight. As confidence grows through monitoring and evaluation, autonomy can be gradually increased. This mirrors how humans gain responsibility over time, such as junior employees earning trust.</p>
<p>Context matters deeply. A customer support agent may operate with high autonomy, while a medical or financial agent requires stricter controls. Designing for balance means accepting trade-offs and making values explicit.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="autonomy-vs-control-trade-off-table">Autonomy vs Control Trade-off Table<a href="#autonomy-vs-control-trade-off-table" class="hash-link" aria-label="Direct link to Autonomy vs Control Trade-off Table" title="Direct link to Autonomy vs Control Trade-off Table" translate="no">​</a></h3>
<table><thead><tr><th>Dimension</th><th>High Autonomy</th><th>High Control</th></tr></thead><tbody><tr><td>Speed</td><td>Fast decisions</td><td>Slower, deliberate</td></tr><tr><td>Risk</td><td>Higher</td><td>Lower</td></tr><tr><td>Scalability</td><td>High</td><td>Limited</td></tr><tr><td>Human Effort</td><td>Low</td><td>High</td></tr></tbody></table>
<hr>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">​</a></h2>
<p>Agentic systems demand a new approach to safety—one that combines technical guardrails, human judgment, continuous monitoring, and thoughtful balance between autonomy and control. Safety risks are inevitable, but harm is not. By designing systems that anticipate failure, constrain behavior, and invite human oversight, we can unlock the benefits of autonomy without sacrificing trust or responsibility.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="reflection-questions">Reflection Questions<a href="#reflection-questions" class="hash-link" aria-label="Direct link to Reflection Questions" title="Direct link to Reflection Questions" translate="no">​</a></h2>
<ol>
<li class="">Which safety risks are most relevant in the domain you are working in, and why?</li>
<li class="">How would you decide which actions require human approval versus full automation?</li>
<li class="">What signals would you monitor to detect unsafe behavior early?</li>
<li class="">How might the balance between autonomy and control change as a system matures?</li>
</ol></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/module-7-evaluation-safety/chapter-2"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Hallucination and Error Handling</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/module-7-evaluation-safety/chapter-4"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Ethical and Responsible Agentic AI</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#safety-risks-in-agentic-systems" class="table-of-contents__link toc-highlight">Safety Risks in Agentic Systems</a><ul><li><a href="#common-categories-of-safety-risks" class="table-of-contents__link toc-highlight">Common Categories of Safety Risks</a></li><li><a href="#analogy-the-overzealous-intern" class="table-of-contents__link toc-highlight">Analogy: The Overzealous Intern</a></li><li><a href="#safety-risk-landscape-table" class="table-of-contents__link toc-highlight">Safety Risk Landscape Table</a></li></ul></li><li><a href="#guardrail-design-patterns" class="table-of-contents__link toc-highlight">Guardrail Design Patterns</a><ul><li><a href="#common-guardrail-patterns" class="table-of-contents__link toc-highlight">Common Guardrail Patterns</a></li><li><a href="#example-financial-trading-agent" class="table-of-contents__link toc-highlight">Example: Financial Trading Agent</a></li><li><a href="#guardrail-comparison-table" class="table-of-contents__link toc-highlight">Guardrail Comparison Table</a></li></ul></li><li><a href="#human-approval-workflows" class="table-of-contents__link toc-highlight">Human Approval Workflows</a><ul><li><a href="#typical-approval-workflow-steps" class="table-of-contents__link toc-highlight">Typical Approval Workflow Steps</a></li><li><a href="#analogy-air-traffic-control" class="table-of-contents__link toc-highlight">Analogy: Air Traffic Control</a></li></ul></li><li><a href="#case-study-human-in-the-loop-in-a-healthcare-scheduling-agent" class="table-of-contents__link toc-highlight">Case Study: Human-in-the-Loop in a Healthcare Scheduling Agent</a><ul><li><a href="#context" class="table-of-contents__link toc-highlight">Context</a></li><li><a href="#problem" class="table-of-contents__link toc-highlight">Problem</a></li><li><a href="#solution" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#results" class="table-of-contents__link toc-highlight">Results</a></li><li><a href="#lessons-learned" class="table-of-contents__link toc-highlight">Lessons Learned</a></li></ul></li><li><a href="#intervention-and-override-mechanisms" class="table-of-contents__link toc-highlight">Intervention and Override Mechanisms</a><ul><li><a href="#types-of-overrides" class="table-of-contents__link toc-highlight">Types of Overrides</a></li></ul></li><li><a href="#monitoring-unsafe-behavior" class="table-of-contents__link toc-highlight">Monitoring Unsafe Behavior</a><ul><li><a href="#monitoring-metrics-table" class="table-of-contents__link toc-highlight">Monitoring Metrics Table</a></li></ul></li><li><a href="#balancing-autonomy-and-control" class="table-of-contents__link toc-highlight">Balancing Autonomy and Control</a><ul><li><a href="#autonomy-vs-control-trade-off-table" class="table-of-contents__link toc-highlight">Autonomy vs Control Trade-off Table</a></li></ul></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#reflection-questions" class="table-of-contents__link toc-highlight">Reflection Questions</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Learning Materials. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>