<!doctype html>
<html lang="id" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-7-evaluation-safety/chapter-2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Hallucination and Error Handling | Learning Materials</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="http://localhost:3000/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="http://localhost:3000/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="http://localhost:3000/module-7-evaluation-safety/chapter-2"><meta data-rh="true" property="og:locale" content="id"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="id"><meta data-rh="true" name="docsearch:language" content="id"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Hallucination and Error Handling | Learning Materials"><meta data-rh="true" name="description" content="Learning Objectives"><meta data-rh="true" property="og:description" content="Learning Objectives"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="http://localhost:3000/module-7-evaluation-safety/chapter-2"><link data-rh="true" rel="alternate" href="http://localhost:3000/module-7-evaluation-safety/chapter-2" hreflang="id"><link data-rh="true" rel="alternate" href="http://localhost:3000/en/module-7-evaluation-safety/chapter-2" hreflang="en"><link data-rh="true" rel="alternate" href="http://localhost:3000/module-7-evaluation-safety/chapter-2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Hallucination and Error Handling","item":"http://localhost:3000/module-7-evaluation-safety/chapter-2"}]}</script><link rel="stylesheet" href="/assets/css/styles.b3bb77c0.css">
<script src="/assets/js/runtime~main.ea5f7858.js" defer="defer"></script>
<script src="/assets/js/main.e790a99d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Lewati ke konten utama"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Lewati ke konten utama</a></div><nav aria-label="Utama" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alihkan bilah sisi" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Learning Materials Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Learning Materials Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Learning Materials</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>Bahasa Indonesia</a><ul class="dropdown__menu"><li><a href="/module-7-evaluation-safety/chapter-2" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="id">Bahasa Indonesia</a></li><li><a href="/en/module-7-evaluation-safety/chapter-2" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Ubah antara modus gelap dan modus terang (saat ini system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Gulir kembali ke atas" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Bilah sisi dokumentasi" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/"><span title="Deep Dive into Agentic AI: Design, Implementation, and Production Systems" class="linkLabel_WmDU">Deep Dive into Agentic AI: Design, Implementation, and Production Systems</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/module-1-introduction-agentic-ai/chapter-1"><span title="Introduction to Agentic AI and Autonomous Systems" class="categoryLinkLabel_W154">Introduction to Agentic AI and Autonomous Systems</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/module-2-core-components/chapter-1"><span title="Core Components of an AI Agent" class="categoryLinkLabel_W154">Core Components of an AI Agent</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/module-3-architectures-patterns/chapter-1"><span title="Agent Architectures and Design Patterns" class="categoryLinkLabel_W154">Agent Architectures and Design Patterns</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/module-4-agent-frameworks/chapter-1"><span title="Building Agents with Modern Frameworks" class="categoryLinkLabel_W154">Building Agents with Modern Frameworks</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/module-5-planning-memory-decision/chapter-1"><span title="Planning, Memory, and Decision-Making" class="categoryLinkLabel_W154">Planning, Memory, and Decision-Making</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/module-6-multi-agent/chapter-1"><span title="Multi-Agent Systems and Collaboration" class="categoryLinkLabel_W154">Multi-Agent Systems and Collaboration</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/module-7-evaluation-safety/chapter-1"><span title="Evaluation, Safety, and Alignment" class="categoryLinkLabel_W154">Evaluation, Safety, and Alignment</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/module-7-evaluation-safety/chapter-1"><span title="Agent Evaluation Metrics and Methods" class="linkLabel_WmDU">Agent Evaluation Metrics and Methods</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/module-7-evaluation-safety/chapter-2"><span title="Hallucination and Error Handling" class="linkLabel_WmDU">Hallucination and Error Handling</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/module-7-evaluation-safety/chapter-3"><span title="Safety, Guardrails, and Human-in-the-Loop" class="linkLabel_WmDU">Safety, Guardrails, and Human-in-the-Loop</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/module-7-evaluation-safety/chapter-4"><span title="Ethical and Responsible Agentic AI" class="linkLabel_WmDU">Ethical and Responsible Agentic AI</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/module-8-scaling-production/chapter-1"><span title="Scaling, Optimization, and Production Deployment" class="categoryLinkLabel_W154">Scaling, Optimization, and Production Deployment</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/module-9-capstone/chapter-1"><span title="Capstone Project: Build an End-to-End Agentic AI System" class="categoryLinkLabel_W154">Capstone Project: Build an End-to-End Agentic AI System</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Runut navigasi"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Halaman utama" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Evaluation, Safety, and Alignment</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Hallucination and Error Handling</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">Pada halaman ini</button></div><div class="theme-doc-markdown markdown"><header><h1>Evaluation, Safety, and Alignment: Hallucination and Error Handling</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Taut langsung ke Learning Objectives" title="Taut langsung ke Learning Objectives" translate="no">​</a></h2>
<ul>
<li class="">Identify hallucination types</li>
<li class="">Detect agent errors</li>
<li class="">Implement mitigation strategies</li>
<li class="">Design fallback mechanisms</li>
<li class="">Analyze error trends</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Taut langsung ke Introduction" title="Taut langsung ke Introduction" translate="no">​</a></h2>
<p>This chapter focuses on identifying and mitigating agent errors.</p>
<hr>
<hr>
<p>As AI agents become more capable and autonomous, they are increasingly trusted with tasks that affect real people, real businesses, and real decisions. From customer support bots and data analysis agents to coding assistants and autonomous research systems, these agents are now embedded deeply into workflows where correctness, reliability, and safety matter. However, alongside this growing capability comes a persistent and sometimes subtle problem: <strong>agent errors</strong>, especially <strong>hallucinations</strong>—confident but incorrect outputs that can mislead users and systems alike.</p>
<p>This chapter focuses on <strong>evaluation, safety, and alignment</strong>, with a particular emphasis on <strong>hallucination and error handling</strong>. Rather than treating errors as rare edge cases, we approach them as an inevitable part of intelligent systems that must be <strong>actively detected, mitigated, logged, and learned from</strong>. A well-designed agent is not one that never makes mistakes, but one that recognizes uncertainty, fails gracefully, and improves continuously.</p>
<p>We will move progressively from understanding the <strong>types of hallucinations</strong>, to learning <strong>how to detect errors</strong>, to implementing <strong>confidence-based mitigation</strong>, <strong>retry and fallback strategies</strong>, and <strong>robust logging and postmortems</strong>. Finally, we will explore <strong>continuous improvement loops</strong> that turn errors into long-term system learning. Throughout the chapter, you will find concrete examples, detailed case studies, tables, and visual diagrams that make abstract safety concepts tangible and practical.</p>
<hr>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="">Identify different types of hallucinations in AI agent outputs</li>
<li class="">Detect agent errors using automated and human-in-the-loop techniques</li>
<li class="">Implement confidence-based mitigation strategies</li>
<li class="">Design robust retry and fallback mechanisms</li>
<li class="">Log errors effectively and conduct meaningful postmortems</li>
<li class="">Analyze error trends to drive continuous improvement</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="types-of-hallucinations">Types of Hallucinations<a href="#types-of-hallucinations" class="hash-link" aria-label="Taut langsung ke Types of Hallucinations" title="Taut langsung ke Types of Hallucinations" translate="no">​</a></h2>
<p>Hallucinations are among the most discussed and misunderstood failure modes of AI agents. At a high level, a hallucination occurs when an agent produces output that <strong>appears coherent and confident but is factually incorrect, logically inconsistent, or unsupported by available evidence</strong>. Importantly, hallucinations are not random noise—they are often <em>plausible</em>, which makes them particularly dangerous in real-world applications.</p>
<p>Historically, the term “hallucination” was borrowed from cognitive science and psychology, where it describes perceptions without external stimuli. In AI, the term emerged as large language models began generating fluent text that sounded authoritative even when it was wrong. Early deployments of conversational AI revealed that users tended to trust confident-sounding answers, amplifying the risk of misinformation.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="factual-hallucinations">Factual Hallucinations<a href="#factual-hallucinations" class="hash-link" aria-label="Taut langsung ke Factual Hallucinations" title="Taut langsung ke Factual Hallucinations" translate="no">​</a></h3>
<p>Factual hallucinations occur when an agent invents or misstates objective facts—dates, numbers, events, references, or entities. These are the most visible and widely recognized hallucinations.</p>
<p>Consider a travel-planning agent that confidently states that a country does not require a visa when it actually does, or a medical assistant that fabricates clinical trial results. These errors often arise because the model is optimizing for <em>plausibility</em>, not truth verification.</p>
<p>Why factual hallucinations matter:</p>
<ul>
<li class="">They can directly mislead users into making harmful decisions</li>
<li class="">They undermine trust in the system over time</li>
<li class="">They are often difficult to detect without external validation</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="logical-and-reasoning-hallucinations">Logical and Reasoning Hallucinations<a href="#logical-and-reasoning-hallucinations" class="hash-link" aria-label="Taut langsung ke Logical and Reasoning Hallucinations" title="Taut langsung ke Logical and Reasoning Hallucinations" translate="no">​</a></h3>
<p>Not all hallucinations involve wrong facts. Some involve <strong>flawed reasoning</strong>, where the steps between premises and conclusions are inconsistent or invalid. The final answer may even be correct by coincidence, but the reasoning path is broken.</p>
<p>An analogy from everyday life is a student who guesses the right answer on a math exam but shows incorrect steps. In AI systems, such hallucinations are dangerous because they can propagate incorrect reasoning patterns into downstream decisions.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="contextual-and-instructional-hallucinations">Contextual and Instructional Hallucinations<a href="#contextual-and-instructional-hallucinations" class="hash-link" aria-label="Taut langsung ke Contextual and Instructional Hallucinations" title="Taut langsung ke Contextual and Instructional Hallucinations" translate="no">​</a></h3>
<p>Contextual hallucinations occur when an agent ignores, misinterprets, or invents constraints from the prompt or environment. For example, an agent instructed to summarize <em>only</em> a provided document may introduce external knowledge that was never included.</p>
<p>Instructional hallucinations often arise in multi-step agent systems where memory, tool outputs, or prior messages are incorrectly recalled or overwritten.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="comparative-overview-of-hallucination-types">Comparative Overview of Hallucination Types<a href="#comparative-overview-of-hallucination-types" class="hash-link" aria-label="Taut langsung ke Comparative Overview of Hallucination Types" title="Taut langsung ke Comparative Overview of Hallucination Types" translate="no">​</a></h3>
<table><thead><tr><th>Hallucination Type</th><th>Description</th><th>Typical Cause</th><th>Risk Level</th></tr></thead><tbody><tr><td>Factual</td><td>Incorrect objective information</td><td>Training data gaps, no retrieval</td><td>High</td></tr><tr><td>Logical</td><td>Invalid reasoning steps</td><td>Weak chain-of-thought alignment</td><td>Medium–High</td></tr><tr><td>Contextual</td><td>Misuse or invention of context</td><td>Prompt misalignment, memory errors</td><td>Medium</td></tr><tr><td>Instructional</td><td>Violating explicit instructions</td><td>Agent orchestration flaws</td><td>Medium</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="conceptual-map-of-hallucination-sources">Conceptual Map of Hallucination Sources<a href="#conceptual-map-of-hallucination-sources" class="hash-link" aria-label="Taut langsung ke Conceptual Map of Hallucination Sources" title="Taut langsung ke Conceptual Map of Hallucination Sources" translate="no">​</a></h3>
<!-- -->
<p>Understanding these types is foundational, because <strong>different hallucinations require different detection and mitigation strategies</strong>. Treating all errors the same leads to overengineering in some places and blind spots in others.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="detection-techniques">Detection Techniques<a href="#detection-techniques" class="hash-link" aria-label="Taut langsung ke Detection Techniques" title="Taut langsung ke Detection Techniques" translate="no">​</a></h2>
<p>Detecting hallucinations is fundamentally about answering one question: <em>How do we know when an agent is wrong?</em> This is surprisingly difficult, especially when the agent’s output is fluent, confident, and superficially reasonable. Detection techniques have evolved alongside AI systems, moving from manual review to automated, multi-layered approaches.</p>
<p>Early AI systems relied heavily on <strong>human validation</strong>, where domain experts reviewed outputs. While effective, this approach does not scale. Modern agent systems instead use <strong>hybrid detection pipelines</strong> that combine heuristics, statistical signals, external tools, and human oversight.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="rule-based-and-heuristic-detection">Rule-Based and Heuristic Detection<a href="#rule-based-and-heuristic-detection" class="hash-link" aria-label="Taut langsung ke Rule-Based and Heuristic Detection" title="Taut langsung ke Rule-Based and Heuristic Detection" translate="no">​</a></h3>
<p>Rule-based detection uses predefined rules to flag suspicious outputs. For example:</p>
<ul>
<li class="">Flagging answers that contain phrases like “I’m not sure, but…” followed by strong claims</li>
<li class="">Detecting impossible dates (e.g., events in the future described as historical)</li>
<li class="">Checking numerical ranges (negative ages, impossible percentages)</li>
</ul>
<p>These rules are simple and transparent, but brittle. They work well for known error patterns but fail when hallucinations are novel or subtle.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="cross-verification-and-tool-based-detection">Cross-Verification and Tool-Based Detection<a href="#cross-verification-and-tool-based-detection" class="hash-link" aria-label="Taut langsung ke Cross-Verification and Tool-Based Detection" title="Taut langsung ke Cross-Verification and Tool-Based Detection" translate="no">​</a></h3>
<p>A more robust approach involves <strong>cross-checking agent outputs against trusted sources</strong>. For example:</p>
<ul>
<li class="">Using retrieval systems to verify factual claims</li>
<li class="">Running code outputs through test suites</li>
<li class="">Validating structured outputs against schemas</li>
</ul>
<p>This is analogous to fact-checking a news article by consulting multiple independent sources.</p>
<!-- -->
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="confidence-and-uncertainty-signals">Confidence and Uncertainty Signals<a href="#confidence-and-uncertainty-signals" class="hash-link" aria-label="Taut langsung ke Confidence and Uncertainty Signals" title="Taut langsung ke Confidence and Uncertainty Signals" translate="no">​</a></h3>
<p>Agents can also self-report uncertainty. Token-level probabilities, entropy measures, or explicit confidence scores can indicate when an output is unreliable. While not perfect, these signals are valuable inputs into broader detection systems.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="human-in-the-loop-detection">Human-in-the-Loop Detection<a href="#human-in-the-loop-detection" class="hash-link" aria-label="Taut langsung ke Human-in-the-Loop Detection" title="Taut langsung ke Human-in-the-Loop Detection" translate="no">​</a></h3>
<p>For high-stakes applications—medical, legal, financial—human oversight remains essential. Rather than reviewing everything, humans are involved selectively:</p>
<ul>
<li class="">Reviewing flagged outputs</li>
<li class="">Auditing random samples</li>
<li class="">Handling edge cases and disputes</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="comparison-of-detection-techniques">Comparison of Detection Techniques<a href="#comparison-of-detection-techniques" class="hash-link" aria-label="Taut langsung ke Comparison of Detection Techniques" title="Taut langsung ke Comparison of Detection Techniques" translate="no">​</a></h3>
<table><thead><tr><th>Technique</th><th>Strengths</th><th>Weaknesses</th><th>Best Use</th></tr></thead><tbody><tr><td>Rule-Based</td><td>Simple, fast</td><td>Brittle</td><td>Known error patterns</td></tr><tr><td>Cross-Verification</td><td>High accuracy</td><td>Cost, latency</td><td>Factual validation</td></tr><tr><td>Confidence Signals</td><td>Cheap, scalable</td><td>Noisy</td><td>Early warning</td></tr><tr><td>Human Review</td><td>High trust</td><td>Expensive</td><td>High-stakes decisions</td></tr></tbody></table>
<!-- -->
<p>Detection is not about eliminating errors entirely, but about <strong>catching enough of them early to prevent harm</strong>.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="confidence-based-mitigation">Confidence-Based Mitigation<a href="#confidence-based-mitigation" class="hash-link" aria-label="Taut langsung ke Confidence-Based Mitigation" title="Taut langsung ke Confidence-Based Mitigation" translate="no">​</a></h2>
<p>Once an error or potential hallucination is detected, the next question is how the system should respond. Confidence-based mitigation focuses on <strong>modulating agent behavior based on uncertainty</strong>, rather than treating all outputs equally.</p>
<p>Humans do this naturally. When we are unsure, we hedge, ask for clarification, or consult others. AI agents should behave similarly.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="understanding-confidence-in-ai-systems">Understanding Confidence in AI Systems<a href="#understanding-confidence-in-ai-systems" class="hash-link" aria-label="Taut langsung ke Understanding Confidence in AI Systems" title="Taut langsung ke Understanding Confidence in AI Systems" translate="no">​</a></h3>
<p>Confidence can be derived from:</p>
<ul>
<li class="">Model-internal signals (probabilities, entropy)</li>
<li class="">External validation success or failure</li>
<li class="">Historical accuracy in similar contexts</li>
</ul>
<p>Importantly, confidence is <strong>contextual</strong>. An agent may be highly confident summarizing a document but less confident predicting future trends.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="mitigation-strategies-based-on-confidence">Mitigation Strategies Based on Confidence<a href="#mitigation-strategies-based-on-confidence" class="hash-link" aria-label="Taut langsung ke Mitigation Strategies Based on Confidence" title="Taut langsung ke Mitigation Strategies Based on Confidence" translate="no">​</a></h3>
<p>Low-confidence outputs can trigger:</p>
<ul>
<li class="">Clarifying questions to the user</li>
<li class="">Softer language (“I may be mistaken, but…”)</li>
<li class="">Requests for additional data</li>
<li class="">Escalation to a human reviewer</li>
</ul>
<p>High-confidence outputs can proceed automatically, improving efficiency.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="practical-example">Practical Example<a href="#practical-example" class="hash-link" aria-label="Taut langsung ke Practical Example" title="Taut langsung ke Practical Example" translate="no">​</a></h3>
<p>Imagine a financial advisory agent:</p>
<ul>
<li class="">High confidence: Summarizing a user’s transaction history</li>
<li class="">Medium confidence: Suggesting budgeting tips</li>
<li class="">Low confidence: Giving tax advice → escalate to human</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="confidence-threshold-table">Confidence Threshold Table<a href="#confidence-threshold-table" class="hash-link" aria-label="Taut langsung ke Confidence Threshold Table" title="Taut langsung ke Confidence Threshold Table" translate="no">​</a></h3>
<table><thead><tr><th>Confidence Level</th><th>Action</th><th>User Experience</th></tr></thead><tbody><tr><td>High</td><td>Auto-respond</td><td>Fast, seamless</td></tr><tr><td>Medium</td><td>Respond with caveats</td><td>Transparent</td></tr><tr><td>Low</td><td>Ask / Escalate</td><td>Safe, slower</td></tr></tbody></table>
<!-- -->
<p>Confidence-based mitigation reduces harm <strong>without paralyzing the system</strong>, striking a balance between safety and usability.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="retry-and-fallback-strategies">Retry and Fallback Strategies<a href="#retry-and-fallback-strategies" class="hash-link" aria-label="Taut langsung ke Retry and Fallback Strategies" title="Taut langsung ke Retry and Fallback Strategies" translate="no">​</a></h2>
<p>Even with detection and mitigation, errors will occur. Retry and fallback strategies define <strong>what the system does next</strong> when something goes wrong. These strategies are critical for resilience.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="retries-when-and-how">Retries: When and How<a href="#retries-when-and-how" class="hash-link" aria-label="Taut langsung ke Retries: When and How" title="Taut langsung ke Retries: When and How" translate="no">​</a></h3>
<p>Retries involve attempting the task again, often with modifications:</p>
<ul>
<li class="">Adjusted prompts</li>
<li class="">Additional context</li>
<li class="">Different tools or models</li>
</ul>
<p>However, blind retries can amplify errors. Effective retries are <strong>informed retries</strong>, guided by the failure reason.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="fallback-mechanisms">Fallback Mechanisms<a href="#fallback-mechanisms" class="hash-link" aria-label="Taut langsung ke Fallback Mechanisms" title="Taut langsung ke Fallback Mechanisms" translate="no">​</a></h3>
<p>Fallbacks provide alternative paths:</p>
<ul>
<li class="">Simpler models or templates</li>
<li class="">Cached answers</li>
<li class="">Human agents</li>
<li class="">Graceful failure messages</li>
</ul>
<p>An analogy is a GPS system that reroutes when a road is blocked rather than insisting on the original path.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="retry-vs-fallback-comparison">Retry vs Fallback Comparison<a href="#retry-vs-fallback-comparison" class="hash-link" aria-label="Taut langsung ke Retry vs Fallback Comparison" title="Taut langsung ke Retry vs Fallback Comparison" translate="no">​</a></h3>
<table><thead><tr><th>Aspect</th><th>Retry</th><th>Fallback</th></tr></thead><tbody><tr><td>Goal</td><td>Fix error</td><td>Avoid error</td></tr><tr><td>Cost</td><td>Medium</td><td>Varies</td></tr><tr><td>Risk</td><td>Repeating mistake</td><td>Reduced capability</td></tr><tr><td>Best Use</td><td>Transient failures</td><td>Systemic uncertainty</td></tr></tbody></table>
<!-- -->
<p>Well-designed systems combine retries and fallbacks, ensuring that users are never left stuck or misled.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="logging-and-postmortems">Logging and Postmortems<a href="#logging-and-postmortems" class="hash-link" aria-label="Taut langsung ke Logging and Postmortems" title="Taut langsung ke Logging and Postmortems" translate="no">​</a></h2>
<p>Errors that are not logged are errors that will happen again. Logging and postmortems turn failures into learning opportunities.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="effective-logging-practices">Effective Logging Practices<a href="#effective-logging-practices" class="hash-link" aria-label="Taut langsung ke Effective Logging Practices" title="Taut langsung ke Effective Logging Practices" translate="no">​</a></h3>
<p>Good logs capture:</p>
<ul>
<li class="">Input prompts and context</li>
<li class="">Agent decisions and confidence</li>
<li class="">Tool calls and responses</li>
<li class="">User feedback</li>
</ul>
<p>Logs should be structured, searchable, and privacy-aware.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="postmortems-learning-from-failure">Postmortems: Learning from Failure<a href="#postmortems-learning-from-failure" class="hash-link" aria-label="Taut langsung ke Postmortems: Learning from Failure" title="Taut langsung ke Postmortems: Learning from Failure" translate="no">​</a></h3>
<p>A postmortem is a structured analysis conducted after a significant error. It focuses on:</p>
<ul>
<li class="">What happened</li>
<li class="">Why it happened</li>
<li class="">How it was detected</li>
<li class="">How it can be prevented</li>
</ul>
<p>Blame-free postmortems encourage honesty and learning.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-log-structure">Example Log Structure<a href="#example-log-structure" class="hash-link" aria-label="Taut langsung ke Example Log Structure" title="Taut langsung ke Example Log Structure" translate="no">​</a></h3>
<table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td>Timestamp</td><td>When the event occurred</td></tr><tr><td>Agent State</td><td>Model, version</td></tr><tr><td>Input</td><td>User request</td></tr><tr><td>Output</td><td>Agent response</td></tr><tr><td>Confidence</td><td>Score</td></tr><tr><td>Outcome</td><td>Success / Error</td></tr></tbody></table>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="continuous-improvement-loops">Continuous Improvement Loops<a href="#continuous-improvement-loops" class="hash-link" aria-label="Taut langsung ke Continuous Improvement Loops" title="Taut langsung ke Continuous Improvement Loops" translate="no">​</a></h2>
<p>The final step is closing the loop: using logged errors and postmortems to <strong>systematically improve the agent over time</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="feedback-as-a-learning-signal">Feedback as a Learning Signal<a href="#feedback-as-a-learning-signal" class="hash-link" aria-label="Taut langsung ke Feedback as a Learning Signal" title="Taut langsung ke Feedback as a Learning Signal" translate="no">​</a></h3>
<p>Feedback can come from:</p>
<ul>
<li class="">Users correcting errors</li>
<li class="">Humans reviewing outputs</li>
<li class="">Automated metrics</li>
</ul>
<p>This feedback feeds into model updates, prompt refinements, and policy changes.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="improvement-cycle">Improvement Cycle<a href="#improvement-cycle" class="hash-link" aria-label="Taut langsung ke Improvement Cycle" title="Taut langsung ke Improvement Cycle" translate="no">​</a></h3>
<!-- -->
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="long-term-benefits">Long-Term Benefits<a href="#long-term-benefits" class="hash-link" aria-label="Taut langsung ke Long-Term Benefits" title="Taut langsung ke Long-Term Benefits" translate="no">​</a></h3>
<p>Continuous improvement leads to:</p>
<ul>
<li class="">Reduced hallucination rates</li>
<li class="">Increased user trust</li>
<li class="">Better alignment with real-world needs</li>
</ul>
<p>This is not a one-time effort but an ongoing discipline.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Taut langsung ke Summary" title="Taut langsung ke Summary" translate="no">​</a></h2>
<p>In this chapter, we explored hallucinations and error handling as core challenges in building safe, aligned AI agents. We examined different types of hallucinations, learned how to detect them using layered techniques, and applied confidence-based mitigation to reduce harm. We designed retry and fallback strategies for resilience, emphasized the importance of logging and postmortems, and closed with continuous improvement loops that turn failures into progress.</p>
<p>The key takeaway is simple but powerful: <strong>errors are inevitable, but unmanaged errors are unacceptable</strong>. A mature AI system is one that anticipates failure, responds intelligently, and learns continuously.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="reflection-questions">Reflection Questions<a href="#reflection-questions" class="hash-link" aria-label="Taut langsung ke Reflection Questions" title="Taut langsung ke Reflection Questions" translate="no">​</a></h2>
<ol>
<li class="">Which type of hallucination do you think is most dangerous in your domain, and why?</li>
<li class="">How would you balance automation and human oversight in a high-stakes agent system?</li>
<li class="">What confidence signals would you trust most, and which would you treat cautiously?</li>
<li class="">How can postmortems be designed to encourage learning rather than blame?</li>
<li class="">What metrics would best indicate that your continuous improvement loop is working?</li>
</ol></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Halaman dokumentasi"><a class="pagination-nav__link pagination-nav__link--prev" href="/module-7-evaluation-safety/chapter-1"><div class="pagination-nav__sublabel">Sebelum</div><div class="pagination-nav__label">Agent Evaluation Metrics and Methods</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/module-7-evaluation-safety/chapter-3"><div class="pagination-nav__sublabel">Berikut</div><div class="pagination-nav__label">Safety, Guardrails, and Human-in-the-Loop</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#types-of-hallucinations" class="table-of-contents__link toc-highlight">Types of Hallucinations</a><ul><li><a href="#factual-hallucinations" class="table-of-contents__link toc-highlight">Factual Hallucinations</a></li><li><a href="#logical-and-reasoning-hallucinations" class="table-of-contents__link toc-highlight">Logical and Reasoning Hallucinations</a></li><li><a href="#contextual-and-instructional-hallucinations" class="table-of-contents__link toc-highlight">Contextual and Instructional Hallucinations</a></li><li><a href="#comparative-overview-of-hallucination-types" class="table-of-contents__link toc-highlight">Comparative Overview of Hallucination Types</a></li><li><a href="#conceptual-map-of-hallucination-sources" class="table-of-contents__link toc-highlight">Conceptual Map of Hallucination Sources</a></li></ul></li><li><a href="#detection-techniques" class="table-of-contents__link toc-highlight">Detection Techniques</a><ul><li><a href="#rule-based-and-heuristic-detection" class="table-of-contents__link toc-highlight">Rule-Based and Heuristic Detection</a></li><li><a href="#cross-verification-and-tool-based-detection" class="table-of-contents__link toc-highlight">Cross-Verification and Tool-Based Detection</a></li><li><a href="#confidence-and-uncertainty-signals" class="table-of-contents__link toc-highlight">Confidence and Uncertainty Signals</a></li><li><a href="#human-in-the-loop-detection" class="table-of-contents__link toc-highlight">Human-in-the-Loop Detection</a></li><li><a href="#comparison-of-detection-techniques" class="table-of-contents__link toc-highlight">Comparison of Detection Techniques</a></li></ul></li><li><a href="#confidence-based-mitigation" class="table-of-contents__link toc-highlight">Confidence-Based Mitigation</a><ul><li><a href="#understanding-confidence-in-ai-systems" class="table-of-contents__link toc-highlight">Understanding Confidence in AI Systems</a></li><li><a href="#mitigation-strategies-based-on-confidence" class="table-of-contents__link toc-highlight">Mitigation Strategies Based on Confidence</a></li><li><a href="#practical-example" class="table-of-contents__link toc-highlight">Practical Example</a></li><li><a href="#confidence-threshold-table" class="table-of-contents__link toc-highlight">Confidence Threshold Table</a></li></ul></li><li><a href="#retry-and-fallback-strategies" class="table-of-contents__link toc-highlight">Retry and Fallback Strategies</a><ul><li><a href="#retries-when-and-how" class="table-of-contents__link toc-highlight">Retries: When and How</a></li><li><a href="#fallback-mechanisms" class="table-of-contents__link toc-highlight">Fallback Mechanisms</a></li><li><a href="#retry-vs-fallback-comparison" class="table-of-contents__link toc-highlight">Retry vs Fallback Comparison</a></li></ul></li><li><a href="#logging-and-postmortems" class="table-of-contents__link toc-highlight">Logging and Postmortems</a><ul><li><a href="#effective-logging-practices" class="table-of-contents__link toc-highlight">Effective Logging Practices</a></li><li><a href="#postmortems-learning-from-failure" class="table-of-contents__link toc-highlight">Postmortems: Learning from Failure</a></li><li><a href="#example-log-structure" class="table-of-contents__link toc-highlight">Example Log Structure</a></li></ul></li><li><a href="#continuous-improvement-loops" class="table-of-contents__link toc-highlight">Continuous Improvement Loops</a><ul><li><a href="#feedback-as-a-learning-signal" class="table-of-contents__link toc-highlight">Feedback as a Learning Signal</a></li><li><a href="#improvement-cycle" class="table-of-contents__link toc-highlight">Improvement Cycle</a></li><li><a href="#long-term-benefits" class="table-of-contents__link toc-highlight">Long-Term Benefits</a></li></ul></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#reflection-questions" class="table-of-contents__link toc-highlight">Reflection Questions</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Learning Materials. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>